Question,Answer
What is "stumblestepping" and how does it weaponize the TDX countermeasure against itself?​,Stumblestepping is a form of single stepping attack against TDX enabled CPU (Intel Xeon ...). The idea is to use the countermeasure of TDX against itself. Basically to avoid single stepping the TDX module will take the control over the hypervisor and will re-enter a random amount of time one instruction at a time the TD. So if we check the cache we can see when the TDX re-enters the TD.
Which structure in the FPGA is targeted and what physical effect generates the response map? (No need to explain the transistor operation),The BBRAM gets attacked and the seedbeck effect in the SRAM cell changes the current of the device. Through the current monitoring the TLS map is generated.
Explain the mechanism by which the described electromagnetic (EM) attack influences the Ring Oscillators (ROs) within the TRNG.,A near‐field electromagnetic probe was used to drive with a continuous‐wave tone at or near the natural oscillation frequency of on‐chip ring oscillators (ROs) capacitively couples into the RO's inverters and injection‐locks them to the external signal. As the probe's RF power is raised into the microwatt‐to‐milliwatt range each RO's intrinsic thermal‐noise‐driven phase jitter is overwhelmed and its free‐running oscillation is forced to track the injected tone. Crucially once one RO is locked all nearby ROs synchronize via shared coupling eliminating their mutual phase differences and destroying the independent entropy sources on which the TRNG relies. The result is a fully deterministic highly biased output stream—despite no physical tampering with power rails or logic—rendering the TRNG's randomness null.
Explain why processing two plaintexts in parallel during bitsliced AES execution does not increase DPA attack complexity?,Processing two plaintext blocks in parallel means for instance that register R1 stores 32 LSBs 16 of one plaintext and 16 of the other. As the key is ﬁxed both plaintext blocks get encrypted under the same key. Making a guess on one key byte we can attack both encryptions at the same time and predict two bits in a register (2 out of 32 instead of 1 out of 16). The power model is hence the Hamming weight (HW) of two bits in a register that are aﬀected by the same sub-key. This observation has an important consequence: processing more plaintext blocks in parallel does not make an attack harder if the adversary is aware of the bitsliced implementation. In fact the ratio of predicted bits and processed bits and hence the ratio of signal to algorithmic noise is constant. Processing multiple plaintexts in parallel using bitslicing does not increase the complexity of a DPA attack because the attacker can still isolate the leakage corresponding to each plaintext independently.
For which conditions are hash-based algorithms preferred to protect embedded devices against PQ adversaries?,Hash-based algorithms are preferred for applications that rely on a long-term public key. In this case a conservative choice of algorithm is needed. The security hash-based algorithms provide is preferable in these cases because of their maturity.
What makes the attack via timing violation in the paper difficult to detect?,The timing violation attack's duration can be well controlled i.e. transisient to be hard to detect. The voltage drop caused is well controlled in closed loops so that it will only trigger timing faults but not reset. Furthermore the victim of the attack is carefully chosen to be TRNG whose output cannot be easily verified in a short time. For example some TRNG is the test failed the statistical tests even in the absence of attack.
How does transient execution improve cache attacks in environments with low resolution timers?,Transient execution lets you sneak in tiny cache operations that survive even when the CPU rolls back a mispredicted branch. By using speculative execution we can create new types of gates that are based on the cache state (1 in cache & 0 not in cache). By chaining those gates into a tree (or even a hypertree) you multiply a single cache hit/miss into a big cascade of hits or misses turning a nanosecond-scale difference into something measurable by a millisecond-level timer. In short speculative execution plants hidden cache breadcrumbs and the amplification trees blow those whispers up into signals coarse timers can hear. This effectively sharpens the apparent resolution of timers.
What are the benefits of using a symmetric encryption structure in the encryption module of CEASER?,"The benefit of using symmetric encryption is that the same hardware can be used for both encryption and decryption. This enables hardware reuse and reduces overall hardware overhead."Here is an overview of the paper to help with understanding:This paper aims to address conflict-based cache attacks by randomizing the mapping between memory addresses and cache sets. Traditional solutions use a random mapping table but that introduces significant hardware overhead. Instead the authors propose CEASE which uses a lightweight 4-stage Feistel Network to compute encrypted addresses directly avoiding the need for a table.However since the mapping remains static an attacker can still learn the eviction set in about 22 seconds. To address this the authors introduce CEASER which adds periodic remapping by changing the encryption key over time. This dramatically increases security — making it virtually impossible for an attacker to learn eviction sets (it will take the attacker over a hundred years).Therefore the system's security mainly relies on remapping not on the cryptographic strength of the cipher itself. That's why the authors choose a low-latency symmetric encryption method — it's fast (2 cycles) efficient and reuses the same hardware for encryption and decryption.(The authors note that the 4-stage Feistel Network provides a strong pseudo-random permutation but just "pseudo-random permutation" which I think is not cryptographically strong in the traditional sense.)
From an attacker's perspective what is the effect of reducing sample rate and bit depth while performing external attack?,Reducing Sample Rate: Makes attacks more challenging but does not necessarily prevent them especially for algorithms like AES with strong repetitive leakage patterns. Even 1:26 down sampling (compare to system clock) could be acceptable matching the realistic case of measuring from ADC.Reducing Bit Depth: (Effective bit depth means the actual number of distinguishable levels present in your measured data.) Could significantly limit the attacker's ability to detect subtle power variations. While some resolution reduction is tolerable too low a bit depth (e.g. 2 bits or ~1.5 effective bits) makes attacks fail even with large numbers of traces. Conclusion:Under sampling does not fully remove information; Reducing ADC resolution degrades attack efficiency. Good dynamic range usage and high bit-depth is important for capturing subtle power variations. We would conclude that an attack is more sensitive to reducing bit depth than down sampling.
Explain with an example how the Intel SGX enclaves can be compromised on servers even when remote attestation checks are performed?,The enclaves are compromised by undervolting the CPU just enough such that a fault is injected but not that much that the CPU resets. To get to this undervolting we need access to the PMBus via the BMC (Board Management Controller) on which we need root access to perform the attack. The root access can be obtained via repacking firmware which enables root shell access to the BMC. This is possible due to a vulnerability in the firmware which enables an attacker to obtain the signing key of the firmware (as only the headers of the file system are encrypted and not the entire file system where the key lives). Once we have root access to the BMC we can start sending commands on the I2C bus. First we need to get the address of the VRM (Voltage Regulator Module) which we can obtain by querying voltages of the different devices which are connected via the I2C bus and then checking if they are above a certain threshold which is required for a VRM. This way we also obtain a base line voltage for the CPU which is used in the undervolting attack. Then we start an operation on the CPU and gradually lower the voltage of the CPU until a fault occurs. These faults can then later be used to recover the private key via e.g. the Lenstra attack. The remote attestation could not detect this attack.
What should a kernel space attacker do to increase the chance of success of this attack?,To have a higher chance of success the quality of the samples collected should be higher.As a kernel level attacker the following points will help in getting the good samples thus increasing the chances of success> Run multiple instances of the victim workload across multiple cores to amplify the average power consumption> Execute each instance of the victim workload N times with same input> Use a lower time window τ configuration for PL
What is a Capability Lookup Table (CLUT) and how does it improve protection from malicious DMA over traditional IOMMU-based methods?,CLUT stands for "Capability Lookup Table" which is a piece of hardware which is populated with CHERI capabilities in a bounded buffer sense. Its entries include capabilities with fine address bounds. The protection itself comes from the capabilities and the CLUT makes sure they can be used in a simple but efficient way. A CLUT is a more efficient holder of capabilities compared to an IOMMU in terms of speed and complexity and memory locations and their properties cannot be accessed by other hardware and software but only by a trustworthy central processor.